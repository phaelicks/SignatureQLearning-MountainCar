{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tabulate\n",
    "\n",
    "# import signature policy and q-learning algorithm\n",
    "import qlearning_method as qlearning\n",
    "from qlearning_policies import SigQFunction\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If results and plots from this notebook should be saved set `save=True` and select an identifiying prefix used for file names when saving results and plots, e.g. current date with a letter or number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "file_prefix = '20240903_A'\n",
    "\n",
    "if save_flag:\n",
    "    # check if files with selected prefix exist\n",
    "    from pathlib import Path\n",
    "    if len(list(Path('../results').glob(file_prefix + '*'))) + \\\n",
    "        len(list(Path('../figures').glob(file_prefix + '*'))) != 0:\n",
    "        file_prefix = input('Files with the chosen prefix already exist.\\nPlease enter new prefix.')\n",
    "    print('Results will be save with chosen prefix:', file_prefix)\n",
    "else:\n",
    "    print('Results will not be saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "steps = 200\n",
    "env = gym.make('MountainCar-v0', max_episode_steps=steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with different base-points\n",
    "\n",
    "We want to perform training runs for base-point configurations at the beginning of an episode. Since at $t=0$, there is only only one observation tuple available to the agent, in the Mountain Car environment considered here this is provided as $o_0 = (p_0, 1)$, with $p_0$ the initial position of the car uniformly sampled from $[-0.6, -0.4]$. \n",
    "\n",
    "We investigate the approach of appending a base-point $o_{-1} = (p_{-1},1)$ to the history prior to $o_0$ and compute the signature based on the two samples $(o_1, o_0)$. We consider different positions for the base-point position $p_{-1}$. Note that with the use of a fixed basepoint for all observation histories, the signature is not translation invariant.\n",
    "\n",
    "Additionally, we investigate the approach of using no base-point and simply setting the signature components to $0$ at $t=0$. The first non-zero values for the signature components are computed at $t=1$. Here, the signature of the observation history is translation invariant.\n",
    "\n",
    "Specify the base-point positions, the configuration of the Signature-Q-Function approximation and the training algorithm hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepoint_positions = [-0.65, -0.50, -0.35, None] # None corresponds to no basepoint\n",
    "training_runs = 4\n",
    "\n",
    "# signature config, basepoint will be added below\n",
    "sigq_params = dict(\n",
    "    in_channels = 2,\n",
    "    out_dimension = 3,\n",
    "    sig_depth = 5,\n",
    "    initial_bias = -0.1\n",
    ")\n",
    "\n",
    "# training config\n",
    "training_params = dict(\n",
    "    episodes = 4000,\n",
    "    discount=0.99,\n",
    "    learning_rate = 0.005,\n",
    "    learning_rate_decay = dict(mode=None), \n",
    "    epsilon = 0.3,\n",
    "    epsilon_decay = dict(mode='linear', end_value=0.05, epochs=2000),\n",
    "    decay_increment = 'success',\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each truncation order, we perform four training runs with all other hyper-parameters held fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_dict = {}\n",
    "\n",
    "for bp_position in basepoint_positions:\n",
    "    basepoint = [bp_position, 1.] if bp_position is not None else None\n",
    "    bp_position_results = []\n",
    "\n",
    "    # run training algorithm loop\n",
    "    pbar = tqdm.trange(training_runs)\n",
    "    for run in pbar:\n",
    "        pbar.set_description(\"Basepoint position: {}  |  Training runs\".format(bp_position, run))\n",
    "        sigQfunction = SigQFunction(initial_basepoint=basepoint, **sigq_params)\n",
    "        run_results = qlearning.train(env, sigQfunction, **training_params) \n",
    "        bp_position_results.append(run_results)      \n",
    "    \n",
    "    train_res_dict[bp_position] = bp_position_results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single run\n",
    "Plot the results for a specific basepoint configuration and run. For each episode over the specified run, the following results are plotted:\n",
    "- the reward obtained, \n",
    "- the total loss occured, \n",
    "- the car's end position, \n",
    "- the steps before reaching the goal or terminating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "bp_position = -0.5 # -0.65, -0.5, -0.35, None\n",
    "run_id = 2\n",
    "\n",
    "basepoint = [bp_position, 1.] if bp_position is not None else None\n",
    "\n",
    "print(\"Basepoint position {} in training run {}: {} Successes\".format(\n",
    "    bp_position, run_id, sum(np.array(train_res_dict[bp_position][run_id][2])>=0.5)\n",
    "))\n",
    "utils.plot_results(train_res_dict[bp_position], run=run_id, ma_window=100,\n",
    "                   title=\"Results for basepoint {}, run {}\".format(basepoint, run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged over all runs for specific basepoint\n",
    "\n",
    "For a fixed basepoint configuration, we average the results over all training runs and calculate the mean and standard deviation of\n",
    "- reward per episode\n",
    "- loss per episode\n",
    "- end position per episode\n",
    "- steps until termination per episode\n",
    "\n",
    "and plot the average results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_position = -0.5 # -0.65, -0.5, -0.35, None\n",
    "\n",
    "# select reward, loss, end position, steps for array\n",
    "results_array = np.array(\n",
    "    [ train_res_dict[bp_position][run][0:4] for run in range(len(train_res_dict[bp_position])) ], \n",
    "    ndmin=3\n",
    ")\n",
    "training_results_means = results_array.mean(axis=0)\n",
    "training_results_stds = results_array.std(axis=0)\n",
    "\n",
    "utils.plot_mean_results(training_results_means, training_results_stds, \n",
    "                        title='Results averaged over training runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Q-values\n",
    "\n",
    "We check whether the Q-values at the beginning of the episode for a specified basepoint configuration converge towards the same value as the overall reward obtained over the episode. This is an indication, that the algorithm learns the true optimal Q-values of the problem. Unfortunately this is not the case here.\n",
    "\n",
    "The first Q-values are plotted in two ways:\n",
    "- The actual first Q-values saved during training for each episode and calculated for the respective observation $o_0$ encountered in each episode.\n",
    "- The first Q-values for a fixed observation at $t=0$, for example $o_0 = (-0.5, 1.)$, calculated with the intermediate Q-functions saved during training each 10 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# choose basepoint position, starting position and run id\n",
    "bp_position = -0.5 # -0.65, -0.5, -0.35, None\n",
    "start_position = -0.5\n",
    "run_id = -1\n",
    "\n",
    "basepoint = [bp_position, 1.] if bp_position is not None else None\n",
    "sigqfunction = SigQFunction(initial_basepoint=basepoint, **sigq_params)\n",
    "\n",
    "# plot with intermediate Q-function approximations\n",
    "utils.plot_first_Q_values(train_res_dict[bp_position], run_id, intermediate_qfunctions=True,\n",
    "                          window=(0, training_params['episodes']), sigq_container=sigqfunction,\n",
    "                          start_position=start_position, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First observation value\n",
    "Additionally we take a look at the value of the history at time $t=0$ for the start position $p_0 = -0.5$, which corresponds to the middle of the interval $[-0.6, -0.4]$ in which the car is placed randomly at the start of the episode. The value is calculated as the Q-values averaged over actions and is given by\n",
    "$$\n",
    "    V_0(\\hat{h}_0) = \\frac{1}{3}\\sum_{i=0}^2 Q(\\hat{h}_0, a_i),   \n",
    "$$\n",
    "It gives the value over an episode following a greedy policy based on the current approximate Q-function $Q$. If $V_0(\\hat{h}_0)$ converges towards the average reward over an episode, this indicates that the algorithm has converged to the true Q-values (at least for the Q-values at the start of the episode). Unfortunatly this is not the case here, since the algorithm over estimates Q-values.\n",
    "\n",
    "The plot displays the mean and standard deviation of $V_0(\\hat{h}_0)$ calculated over all performed training runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose basepoint position \n",
    "bp_position = -0.5 # -0.65, -0.5, -0.35, None\n",
    "\n",
    "# create array containing saved Q-values for all runs\n",
    "first_observations_array = np.array(\n",
    "    [train_res_dict[bp_position][run][-3] for run in range(training_runs)]\n",
    ")\n",
    "value_means = first_observations_array.mean(axis=0)\n",
    "values_stds = first_observations_array.std(axis=0)\n",
    "\n",
    "# plot and save\n",
    "utils.plot_first_obs_value(value_means, values_stds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the final Q-approximations for each basepoint configuration and from each training run over a number of episodes and report statistics. \n",
    "\n",
    "Checkpoints of the `signatureQFunction` approximations during training was saved every 10 episodes and after the last training episode in each training run. The final learned approximation corresponds to the last checkpoint, given as `state_dict` of the `SigQFunction` class instance at the checkpoint time. Other approximation from training may be tested by selecting the appropriate checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of test episodes and Q-function checkpoint\n",
    "test_episodes = 500\n",
    "sigq_checkpoint_id = -1\n",
    "\n",
    "test_res_dict = {}\n",
    "for bp_position in basepoint_positions:\n",
    "    basepoint = [bp_position, 1.] if bp_position is not None else None\n",
    "    sigqfunction = SigQFunction(initial_basepoint=basepoint, **sigq_params)\n",
    "    training_results = train_res_dict[bp_position]\n",
    "    test_results = []\n",
    "\n",
    "    pbar = tqdm.trange(training_runs)\n",
    "    for run in pbar:\n",
    "        pbar.set_description('Basepoint position:  {}  |  Test runs'.format(bp_position))\n",
    "        # load last Sig-Q approximation parameters\n",
    "        sigqfunction.load_state_dict(training_results[run][-1][sigq_checkpoint_id])\n",
    "        results = qlearning.test_multiple_episodes(env, sigqfunction, \n",
    "                                                   test_episodes, epsilon=0.0)\n",
    "        test_results.append(results)\n",
    "    \n",
    "    test_res_dict[bp_position] = test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test result statistics\n",
    "\n",
    "We display statistics as one table for each truncation order. This output format is not the best since we want to compare performance over truncation orders, however it will do to transfer it into a different format in the thesis document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_dict = {}\n",
    "cols = ['Mean\\nreward', 'Std Dev\\nreward', 'Successes', 'Mean\\nsteps', 'Std Dev\\nsteps', 'Min\\nsteps', 'Max\\nsteps', 'Mean first\\nobs value']\n",
    "rows = ['Run {}'.format(i) for i in range(training_runs)]\n",
    "\n",
    "for bp_position in basepoint_positions:\n",
    "    bp_position_stats = []\n",
    "    for run in test_res_dict[bp_position]:\n",
    "        run_array = np.array(run[0:5])\n",
    "        run_stats = []\n",
    "        run_stats.append(run_array[0].mean()) # mean reward\n",
    "        run_stats.append(run_array[0].std()) # std reward\n",
    "        run_stats.append(100*run_array[1].sum()/test_episodes) # percentage successes\n",
    "        run_stats.append(run_array[2].mean()) # mean episode length\n",
    "        run_stats.append(run_array[2].std()) # std episode length\n",
    "        run_stats.append((int(run_array[2].min()))) # min episode steps\n",
    "        run_stats.append(int(run_array[2].max())) # max episode steps\n",
    "        run_stats.append(run_array[4].mean()) # first observation value\n",
    "        \n",
    "        bp_position_stats.append(run_stats)\n",
    "\n",
    "    test_stats_dict[bp_position] = bp_position_stats\n",
    "    print(f'\\n\\nTest statistic for basepoint position {bp_position}:\\n')\n",
    "    print(tabulate.tabulate(bp_position_stats, headers=cols, showindex=rows,floatfmt='.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting position vs. steps until termination for all basepoint configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "\n",
    "# start position vs. number of steps\n",
    "for bp_position in basepoint_positions:\n",
    "    plt.figure(figsize=(5.5, 4.125))        \n",
    "    plt.scatter(test_res_dict[bp_position][run_id][3], \n",
    "                test_res_dict[bp_position][run_id][2], \n",
    "                marker='x')\n",
    "    plt.xlabel(\"Start position\", fontsize=11)\n",
    "    plt.ylabel(\"Number of steps\", fontsize=11)\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.title(' ', fontsize=12) # some space above plot\n",
    "    plt.tight_layout()\n",
    "    file_path = '../figures/{}_start_vs_steps_bp_position_{}.png'.format(file_prefix, bp_position)\n",
    "    if save_flag:\n",
    "        plt.savefig(file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting position vs. reward for all basepoint configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "\n",
    "# start position vs. rewards\n",
    "for bp_position in basepoint_positions:\n",
    "    plt.figure(figsize=(5.5, 4.125))        \n",
    "    plt.scatter(test_res_dict[bp_position][run_id][3], \n",
    "                test_res_dict[bp_position][run_id][0], \n",
    "                marker='x')\n",
    "    plt.xlabel(\"Start position\", fontsize=11)\n",
    "    plt.ylabel(\"Number of steps\", fontsize=11)\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.title(' ', fontsize=12) # some space above plot\n",
    "    plt.tight_layout()\n",
    "    file_path = '../figures/{}_start_vs_steps_bp_position_{}.png'.format(file_prefix, bp_position)\n",
    "    if save_flag:\n",
    "        plt.savefig(file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain an even more qualitative insight into the policy derived from the learned Q-function approximation, we may plot the observation trajectory for a specific truncation order, run and episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_position, run_id, episode_id = None, 0, 70\n",
    "# observation history, (position, time)\n",
    "plt.plot(test_res_dict[bp_position][run_id][-1][episode_id])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_flag:\n",
    "    # create file name and write results\n",
    "    file_path = '../results/' + file_prefix + '_basepoint_results.pkl'\n",
    "    data_to_save = dict(basepoint_positions=basepoint_positions,\n",
    "                        sigq_params = sigq_params, \n",
    "                        training_params=training_params,\n",
    "                        train_res_dict=train_res_dict, \n",
    "                        test_res_dict=test_res_dict)\n",
    "    with open(file_path, 'wb') as f:  # open text file\n",
    "        pickle.dump(data_to_save, f) # serialize the dict\n",
    "    print(\n",
    "        'Training, test results, parameter configuration saved under\\n\\\"{}\\\".'.format(\n",
    "            file_path\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results\n",
    "\n",
    "Instead of performing new training and testing runs, prior saved training and testing results, together with the parameter configuration used in training, may be loaded. Saved results can be found in `../results/` and are identified by a distinct `file_prefix`, e.g. the date the runs were performed, in the format `YYYYMMDD`, together with an upper case letter which enumerates the result saved on the same date. To load results select the respective `file_prefix` of the results to be loaded and set `execute_cell_flag = True`.\n",
    "\n",
    "The following objects are loaded:\n",
    "- `truncation_orders` - list containing the tested signature truncation orders\n",
    "- `sigq_params` - dict containing the signature-Q-function parameters\n",
    "- `training_params` - dict containing the training algorithm parameters\n",
    "- `train_res_dict` - dict, contains training run results for each truncation order\n",
    "- `test_res_dict` - dict, contains test run results for each truncation order\n",
    "â€“ `training_runs`- int, number of performed training runs per truncation order\n",
    "\n",
    "**Note:** To display results after loading them, the respective cells above this section need to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, test results, parameter configuration loaded from: \n",
      " \"../results/20240903_A_basepoint_results.pkl\"\n",
      "\n",
      "Number of training runs: 4.\n",
      "With parameters:\n",
      "\n",
      "{'basepoint_positions': [-0.65, -0.5, -0.35, None],\n",
      " 'sigq_params': {'in_channels': 2,\n",
      "                 'initial_bias': 0.1,\n",
      "                 'out_dimension': 3,\n",
      "                 'sig_depth': 5},\n",
      " 'training_params': {'batch_size': 1,\n",
      "                     'decay_increment': 'success',\n",
      "                     'discount': 0.99,\n",
      "                     'episodes': 4000,\n",
      "                     'epsilon': 0.3,\n",
      "                     'epsilon_decay': {'end_value': 0.05,\n",
      "                                       'epochs': 2000,\n",
      "                                       'mode': 'linear'},\n",
      "                     'learning_rate': 0.005,\n",
      "                     'learning_rate_decay': {'mode': None}}}\n"
     ]
    }
   ],
   "source": [
    "execute_cell_flag = False\n",
    "\n",
    "load_file_prefix = '20240903_A'\n",
    "file_path = '../results/' + load_file_prefix + '_basepoint_results.pkl'\n",
    "\n",
    "if execute_cell_flag:\n",
    "    # set date and id of saved results to load\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_data_dict = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    basepoint_positions = loaded_data_dict['basepoint_positions']\n",
    "    sigq_params = loaded_data_dict['sigq_params']\n",
    "    training_params = loaded_data_dict['training_params']\n",
    "    train_res_dict = loaded_data_dict['train_res_dict']\n",
    "    test_res_dict = loaded_data_dict['test_res_dict']\n",
    "    training_runs = len(list(train_res_dict.values())[0])\n",
    "\n",
    "    print('Training, test results, parameter configuration loaded from: \\n \\\"{}\\\"\\n'.format(file_path))\n",
    "    print('Number of training runs: {}.\\nWith parameters:\\n'.format(training_runs))\n",
    "    pprint({key:loaded_data_dict[key] for key in loaded_data_dict if key not in ('train_res_dict', 'test_res_dict')})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
