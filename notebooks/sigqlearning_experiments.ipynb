{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from pprint import pp as pprint\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import tabulate\n",
    "\n",
    "# import signature q-function and q-learning algorithm\n",
    "import qlearning_method as qlearning\n",
    "from qlearning_policies import SigQFunction\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If results and plots from this notebook should be saved set `save=True` and select an identifiying prefix used for file names when saving results and plots, e.g. current date with a letter or number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = False\n",
    "file_prefix = '20240905_A'\n",
    "\n",
    "# check if files with selected prefix exist\n",
    "from pathlib import Path\n",
    "if len(list(Path('../results').glob(file_prefix + '*'))) + \\\n",
    "    len(list(Path('../figures').glob(file_prefix + '*'))) != 0:\n",
    "    file_prefix = input('Files with the chosen prefix already exist.\\nPlease enter new prefix.')\n",
    "print('Chosen prefix:', file_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 200\n",
    "env = gym.make('MountainCar-v0', max_episode_steps=steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Q-functions\n",
    "\n",
    "We create the environment and specify the configuration of the Signature-Q-Function approximation and the training algorithm hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature config\n",
    "sigq_params = dict(\n",
    "    in_channels = 2, # time is added manually, no actions\n",
    "    out_dimension = 3,\n",
    "    sig_depth = 5,\n",
    "    initial_basepoint = [-0.65, 1.], \n",
    "    initial_bias = 0.1\n",
    ")\n",
    "\n",
    "# training config\n",
    "training_params = dict(\n",
    "    episodes = 4000,\n",
    "    discount=0.99,\n",
    "    learning_rate = 0.05,\n",
    "    learning_rate_decay = dict(mode='exponential', factor=0.995, end_value=1e-05),\n",
    "        #dict(mode=None),\n",
    "        #dict(mode='linear', end_value=1e-05, epochs=4000),\n",
    "    epsilon = 0.3,\n",
    "    epsilon_decay = dict(mode='linear', end_value=0.05, epochs=3000), \n",
    "        #dict(mode=None),\n",
    "    decay_increment = 'success',\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_runs = 10\n",
    "\n",
    "training_results = []\n",
    "final_Q_functions = []\n",
    "\n",
    "# run training algorithm loop\n",
    "pbar = tqdm.trange(training_runs)\n",
    "for run in pbar:\n",
    "    pbar.set_description('Training runs')\n",
    "    sigQfunction = SigQFunction(**sigq_params)\n",
    "    results = qlearning.train(env, sigQfunction, **training_params) \n",
    "    training_results.append(results)\n",
    "    final_Q_functions.append(sigQfunction.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single run\n",
    "We plot the results for a specific run, set by ``run_id``. The results of interest are the reward obtained, the total loss occured, the car's end position, and the steps before reaching the goal or terminating, in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_id = -1\n",
    "print('Number of successes in training run {}: {}'.format(\n",
    "    run_id,sum(np.array(training_results[run_id][2])>=0.5)\n",
    "))\n",
    "utils.plot_results(training_results, run=run_id, ma_window=100,\n",
    "                   title=\"Results for training run {}\".format(run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged over all runs\n",
    "\n",
    "We average the results over all training runs and calculate the mean and standard deviation of\n",
    "- reward per episode\n",
    "- loss per episode\n",
    "- end position per episode\n",
    "- steps until termination per episode\n",
    "\n",
    "and plot the average results. Additionally we create a plot for each of the presented statistics and save them in ``../figures``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select reward, loss, end position, steps for array\n",
    "results_array = np.array(\n",
    "    [ training_results[run][0:4] for run in range(len(training_results)) ], ndmin=3\n",
    ")\n",
    "training_results_means = results_array.mean(axis=0)\n",
    "training_results_stds = results_array.std(axis=0)\n",
    "\n",
    "utils.plot_mean_results(training_results_means, training_results_stds, \n",
    "                        title='Results averaged over training runs')\n",
    "if save_flag:\n",
    "    utils.save_mean_results_plots(training_results_means, training_results_stds,\n",
    "                                  file_path='../figures/', file_id=file_prefix, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Q-values\n",
    "\n",
    "We check whether the Q-values at the beginning of the episode for converge towards the same value as the overall reward obtained over the episode. This is an indication, that the algorithm learns the true optimal Q-values of the problem. Unfortunately this is not the case here.\n",
    "\n",
    "The first Q-values are plotted in two ways:\n",
    "- The actual first Q-values saved during training for each episode and calculated for the respective observation $o_0$ encountered in each episode.\n",
    "- The first Q-values for a fixed observation at $t=0$, for example $o_0 = (-0.5, 1.)$, calculated with the intermediate Q-functions saved during training each 10 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# select run and if plot should be saved\n",
    "run_id = -1\n",
    "sigqfunction = SigQFunction(**sigq_params)\n",
    "\n",
    "# plot and save\n",
    "utils.plot_first_Q_values(training_results, run_id, step=10, \n",
    "                          window=(0,training_params['episodes']),\n",
    "                          show=True, save=save_flag, \n",
    "                          file_path='../figures/{}_first_Q_values.png'.format(file_prefix))\n",
    "\n",
    "start_pos = -0.4\n",
    "utils.plot_first_Q_values(training_results, run_id, intermediate_qfunctions=True,\n",
    "                          window=(1000,training_params['episodes']), time='down',\n",
    "                          sigq_container=sigqfunction, start_position=start_pos, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First observation value\n",
    "Additionally we take a look at the value of the history at time $t=0$ for the start position $p_0 = -0.5$, which corresponds to the middle of the interval $[-0.6, -0.4]$ in which the car is placed randomly at the start of the episode. The value is calculated as the Q-values averaged over actions and is given by\n",
    "$$\n",
    "    V_0(\\hat{h}_0) = \\frac{1}{3}\\sum_{i=0}^2 Q(\\hat{h}_0, a_i),   \n",
    "$$\n",
    "It gives the value over an episode following a greedy policy based on the current approximate Q-function $Q$. If $V_0(\\hat{h}_0)$ converges towards the average reward over an episode, this indicates that the algorithm has converged to the true Q-values (at least for the Q-values at the start of the episode). Unfortunatly this is not the case here, since the algorithm over estimates Q-values.\n",
    "\n",
    "The plot displays the mean and standard deviation of $V_0(\\hat{h}_0)$ calculated over all performed training runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array containing saved values for all run\n",
    "first_observations_array = np.array(\n",
    "    [training_results[run][-3] for run in range(len(training_results))]\n",
    ")\n",
    "value_means = first_observations_array.mean(axis=0)\n",
    "values_stds = first_observations_array.std(axis=0)\n",
    "\n",
    "# plot and save\n",
    "utils.plot_first_obs_value(value_means, values_stds, save=save_flag, \n",
    "                           file_path='../figures/{}first_values.png'.format(file_prefix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single episode\n",
    "\n",
    "We test the learned Q-approximation for a single episode and plot the observation history. The final Q-approximation, or some intermediate approximation saved every 10 steps during training, may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "run_id = -1\n",
    "intermediate_sigq_id = -1\n",
    "sigqfunction = SigQFunction(**sigq_params)\n",
    "sigqfunction.load_state_dict(training_results[run_id][-1][intermediate_sigq_id])\n",
    "\n",
    "# single test episode\n",
    "reward, success, steps, start, first_obs_value, history = qlearning.test_single_episode(\n",
    "    env, sigqfunction, epsilon=0.0, render=False\n",
    ")\n",
    "print(f\"Success: {success}\")\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple episodes\n",
    "\n",
    "We test the final Q-approximations from each training run over a number of episodes and report statistics. \n",
    "\n",
    "Checkpoints of the `signatureQFunction` approximations during training was saved every 10 episodes and after the last training episode in each training run. The final learned approximation corresponds to the last checkpoint, given as `state_dict` of the `SigQFunction` class instance at the checkpoint time. Other approximation from training may be tested by selecting the appropriate checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of test episodes and Q-function checkpoint\n",
    "test_episodes = 500\n",
    "sigq_checkpoint_id = -1\n",
    "\n",
    "sigqfunction = SigQFunction(**sigq_params)\n",
    "test_results = []\n",
    "\n",
    "pbar = tqdm.trange(training_runs)\n",
    "for run in pbar:\n",
    "    pbar.set_description(\"Test runs\".format(run))\n",
    "    # load last Sig-Q approximation\n",
    "    sigqfunction.load_state_dict(training_results[run][-1][sigq_checkpoint_id])\n",
    "    results = qlearning.test_multiple_episodes(env, sigqfunction, \n",
    "                                               test_episodes, epsilon=0.0)\n",
    "    test_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results\n",
    "#### Statistics\n",
    "\n",
    "We calculate the following statistics for each test run:\n",
    "- Mean reward and standard deviation\n",
    "- Number of successes \n",
    "- Mean episode length and standard deviation\n",
    "- Minimum / maximum episode steps\n",
    "- Mean starting position\n",
    "- Mean first observation value\n",
    "\n",
    "Additionally, we plot the episode length vs. starting position for a specified run, to gain a more qualitative insight into the solution that was learned by this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = []\n",
    "cols = ['Mean\\nreward', 'Std\\nreward', 'Successes', 'Mean\\nsteps', 'Std\\nsteps', 'Min\\nsteps', 'Max\\nsteps', 'Mean first\\nobs value']\n",
    "rows = ['Run {}'.format(i) for i in range(len(test_results))]\n",
    "\n",
    "for test_run in test_results:\n",
    "    run_array = np.array(test_run[0:5])\n",
    "    run_stats = []\n",
    "    run_stats.append(run_array[0].mean()) # mean reward\n",
    "    run_stats.append(run_array[0].std()) # std reward\n",
    "    run_stats.append(int(run_array[1].sum())) # number successes\n",
    "    run_stats.append(run_array[2].mean()) # mean episode length\n",
    "    run_stats.append(run_array[2].std()) # std episode length\n",
    "    run_stats.append(int(run_array[2].min())) # min / max episode steps\n",
    "    run_stats.append(int(run_array[2].max())) # max episode steps\n",
    "    run_stats.append(run_array[4].mean()) # first observation value\n",
    "    \n",
    "    test_stats.append(run_stats)\n",
    "\n",
    "print(tabulate.tabulate(test_stats, headers=cols, showindex=rows, floatfmt='.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence intervals\n",
    "We report confidence intervals for rewards per test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals\n",
    "reward_conf_int = []\n",
    "reward_means = []\n",
    "mean_std_error = 0\n",
    "\n",
    "for result in test_results:\n",
    "    mean = np.mean(result[0])\n",
    "    standard_error = stats.sem(result[0])\n",
    "    confidence_interval = stats.norm.interval(0.95, loc=mean, scale=stats.sem(result[0]))\n",
    "    reward_conf_int.append(confidence_interval)\n",
    "    reward_means.append(mean)\n",
    "    mean_std_error += standard_error/len(test_results)\n",
    "\n",
    "print(f'\\nMean std error of mean reward: {mean_std_error:.4f}')\n",
    "\n",
    "# plot confidence intervals\n",
    "fig_id = '20240905_A'\n",
    "for (lower, upper), mean, y in zip(reward_conf_int, reward_means, range(training_runs)):\n",
    "    plt.plot((lower, upper),(y,y),'b|-')\n",
    "    plt.plot(mean, y, 'bo')\n",
    "plt.yticks(range(training_runs), [f'Run {run+1}' for run in range(training_runs)], fontsize=11)\n",
    "plt.xticks(fontsize=11) \n",
    "plt.figsize=(5.5, 4.125)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../figures/{fig_id}_reward_confidence_intervals.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplots\n",
    "run_id = -1\n",
    "\n",
    "# start position vs. number of steps\n",
    "plt.figure(figsize=(5.5, 4.125))        \n",
    "plt.scatter(test_results[run_id][3], test_results[run_id][2], marker='x')\n",
    "plt.xlabel(\"Start position\", fontsize=11)\n",
    "plt.ylabel(\"Number of steps\", fontsize=11)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/{}_start_pos_vs_steps.png'.format(file_prefix))\n",
    "plt.show()\n",
    "\n",
    "# start position vs. reward\n",
    "plt.figure(figsize=(5.5, 4.125))        \n",
    "plt.scatter(test_results[run_id][3], test_results[run_id][0], marker='x')\n",
    "plt.xlabel(\"Start position\", fontsize=11)\n",
    "plt.ylabel(\"Reward\", fontsize=11)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/{}_start_pos_vs_reward.png'.format(file_prefix))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = -1\n",
    "\n",
    "# number of steps vs. reward\n",
    "plt.scatter(test_results[run_id][2], test_results[run_id][0], marker='x')\n",
    "plt.xlabel(\"Number of steps\", fontsize=11)\n",
    "plt.ylabel(\"Reward\", fontsize=11)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# start position vs. end position\n",
    "end_pos = [test_results[run_id][5][i][-1,0] for i in range(len(test_results[run_id][5]))]\n",
    "plt.scatter(test_results[run_id][3], end_pos, marker='x')\n",
    "plt.xlabel(\"Start position\", fontsize=11)\n",
    "plt.ylabel(\"End position\", fontsize=11)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_flag:\n",
    "    # create file name and write results\n",
    "    file_path = '../results/' + file_prefix + '_fixed_setup_results.pkl'\n",
    "    results_dict = dict(sigq_params=sigq_params, \n",
    "                        training_params=training_params, \n",
    "                        training_results=training_results,\n",
    "                        test_results=test_results)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(results_dict, f) # serialize the list\n",
    "    print('Results saved under \\\"{}\\\"'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results\n",
    "\n",
    "Instead of performing new training and testing runs, prior saved training and testing results, together with the parameter configuration used in training, may be loaded. Saved results can be found in `../results/` and are identified by a distinct `file_prefix`, e.g. the date the runs were performed, in the format `YYYYMMDD`, together with an upper case letter which enumerates the result saved on the same date. To load results select the respective `file_prefix` of the results to be loaded and set `execute_cell_flag = True`.\n",
    "\n",
    "The following objects are loaded:\n",
    "- `training_runs`- int, number of performed training and test runs \n",
    "- `sigq_params` - dict containing the signature-Q-function parameters\n",
    "- `training_params` - dict containing the training algorithm parameters\n",
    "- `training_results` - list, entries are lists with training results of each run\n",
    "- `test_results` - list,  entries are lists with test results of each run\n",
    "\n",
    "**Note:** To display results after loading them, the respective cells above this section need to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_cell_flag = False\n",
    "\n",
    "# set the file_prefix of the results to be loaded\n",
    "load_file_prefix = '20240905_A'\n",
    "\n",
    "if execute_cell_flag:\n",
    "    file_path = '../results/' + load_file_prefix + '_fixed_setup_results.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    sigq_params = results_dict[\"sigq_params\"]\n",
    "    training_params = results_dict['training_params']\n",
    "    training_results = results_dict[\"training_results\"]\n",
    "    test_results = results_dict[\"test_results\"]\n",
    "    training_runs = len(results_dict[\"training_results\"])\n",
    "    #comment_dict=results_dict['comment_dict']\n",
    "    print('Training results from \\\"{}\\\" loaded.\\n'.format(file_path))\n",
    "    print('Training runs: {}.\\nWith parameters:\\n'.format(training_runs))\n",
    "    pprint({key:results_dict[key] for key in results_dict if key not in ('training_results', 'test_results')})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
